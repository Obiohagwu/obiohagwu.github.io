---
layout: post
title:  "A computationalist view of selection"
date:   2022-06-19 14:51:19 -0400
categories: jekyll update
---
<br>Gradient descent</br> - in particular, Stochastic gradient descent, in the context of human evolution can be thought of as analogous to natural selection. Given the large dataset that would seemingly encompass genomic scale data for multiple species, we would not want to use a simple gradient descent algorithm as it would tend to scale linearly, growing in size and complexity at every independent iteration, spurring ostensibly higher dimensional tensors that would encapsulate all of the emergent genetic diversity. One could see how a stochastic approach would be a more efficient means for nature to achieve optimal selection as efficiency is highly valuable to reduce entropy in our closed ecological system engine.

We should probably define the key terms for those unfamiliar. Gradient descent is a mathematical optimization method that entails iteratively backpropagating through the negative gradient of an objective function, until a local minimum is reached upon convergence over the highly non-convex space of said negative objective function, indicating in most cases, an optimal theta, which represents the weights being searched for, have been closely approximated.
